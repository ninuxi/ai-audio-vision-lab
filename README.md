# üéõÔ∏è AI Audio Vision Lab

![Python](https://img.shields.io/badge/python-3.8%2B-blue.svg)
![Raspberry Pi](https://img.shields.io/badge/Raspberry%20Pi-4-green)
![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
![AI](https://img.shields.io/badge/AI-PyTorch%20%2B%20Magenta-orange)

> **An innovative AI system that transforms vision into music**  
> Real-time object recognition and coherent music generation, running completely offline on Raspberry Pi 4.

---

## üé• Demo in Action

[![Demo Video](https://img.shields.io/badge/‚ñ∂Ô∏è-Watch%20Demo-red?style=for-the-badge)](https://youtu.be/your-demo-link)

| Detected Object | Generated Style | Audio Sample |
|-----------------|----------------|--------------|
| üå± Plant | Ambient, Relaxing | [‚ñ∂Ô∏è Listen](examples/plant_music.mp3) |
| üìö Book | Classical, Contemplative | [‚ñ∂Ô∏è Listen](examples/book_music.mp3) |
| ‚òï Cup | Jazz, Intimate | [‚ñ∂Ô∏è Listen](examples/cup_music.mp3) |

---

## üß† System Architecture

```mermaid
graph LR
    A[Raspberry Pi Camera] --> B[PyTorch Object Detection]
    B --> C[Semantic Mapping Engine]
    C --> D[Magenta Music Generation]
    D --> E[TensorFlow Lite Inference]
    E --> F[MIDI/Audio Output]
    
    style A fill:#ff9999
    style F fill:#99ff99
```

### üîß Technology Stack

- **Computer Vision**: PyTorch + TorchVision (Optimized MobileNet V2)
- **AI Music**: Google Magenta converted to TensorFlow Lite
- **Hardware**: Raspberry Pi 4, Camera Module v2
- **Audio**: pretty_midi + FluidSynth for real-time synthesis
- **Optimizations**: INT8 quantization, Asynchronous pipeline

---

## üìä Performance on Raspberry Pi 4

| Metric | Value |
|---------|--------|
| **Detection FPS** | 12-15 fps |
| **Generation Latency** | < 2 seconds |
| **RAM Usage** | ~1.4GB |
| **CPU Load** | 65-75% |
| **Boot Time** | ~15 seconds |

---

## üéµ Musical Output Examples

### Semantic Object ‚Üí Music Mapping

The system uses a proprietary algorithm to map visual features into musical parameters:

```python
# Conceptual example (proprietary implementation not public)
def object_to_music_params(detected_object):
    """
    Converts detected objects into musical parameters
    Proprietary logic not disclosed
    """
    semantic_features = extract_semantic_features(detected_object)
    musical_params = {
        'tempo': map_to_tempo(semantic_features.energy),
        'key': map_to_key(semantic_features.emotion),
        'instruments': select_instruments(semantic_features.category)
    }
    return musical_params
```

### üéº Generated Compositions

**Object: Potted Plant** üå±
- **Style**: Ambient, New Age
- **Key**: C Major
- **Tempo**: 72 BPM
- **Instruments**: Synth pads, Soft strings

**Object: Open Book** üìñ
- **Style**: Neoclassical
- **Key**: A minor
- **Tempo**: 60 BPM
- **Instruments**: Piano, String quartet

---

## üöÄ Setup and Installation

### Hardware Requirements
- Raspberry Pi 4 (4GB RAM minimum)
- MicroSD 32GB+ (Class 10)
- Camera Module v2 or USB Camera
- USB Speaker or 3.5mm Jack

### Quick Installation
```bash
# Clone demo repository
git clone https://github.com/ninuxi/ai-audio-vision-lab.git
cd ai-audio-vision-lab

# Install dependencies
pip3 install -r requirements.txt

# Configure hardware
sudo raspi-config  # Enable Camera

# Start demo
python3 demo/vision_music_demo.py
```

---

## üî¨ Research and Development

### Original Technical Contributions

1. **Edge Computing Optimized Pipeline**
   - Custom quantization of Magenta models
   - Circular buffer for real-time processing
   - Intelligent memory mapping for Raspberry Pi

2. **Semantic Mapping Algorithm**
   - Object-emotion correlation based on cognitive research
   - Multi-dimensional musical parameterization
   - Temporal coherence system for smooth transitions

3. **Offline Inference Framework**
   - Zero cloud dependencies
   - Fully embedded models
   - Guaranteed <2s latency

### üìà Future Roadmap

- [ ] **Mobile Version**: Android/iOS porting
- [ ] **Multi-Modal**: Audio input + Visual input
- [ ] **Personalized Learning**: User preference adaptation
- [ ] **ESP32 Port**: Ultra-compact version with TinyML

---

## ü§ù Collaborations and Contact

**Interested in collaborating?** This project is open to:

- üéì **Researchers** in AI/Music Information Retrieval
- üéµ **Musicians** interested in creative technologies
- üíª **Developers** with edge computing experience
- üè¢ **Companies** for commercial applications

### üìß Contact
- **Email**: oggettosonoro@gmail.com  
- **GitHub**: [@ninuxi](https://github.com/ninuxi)
- **Portfolio**: [Complete portfolio link]

---

## ‚öñÔ∏è License and Usage

This repository contains a **demonstration version** of the AI Audio Vision Lab project.  

- ‚úÖ **Demo and examples**: Freely usable (MIT License)
- ‚ùå **Complete source code**: Proprietary, not public
- ü§ù **Commercial collaborations**: Contact for specific licenses

> **Note**: Core algorithms and trained models represent original research and are not publicly available. For full access or commercial partnerships, contact the author directly.

---

## üåü Acknowledgments

Project developed by **Antonio Mainenti** (2024-2025)

*If this project inspires you, leave a ‚≠ê and share it!*

---

**¬© 2025 Antonio Mainenti - Some rights reserved**

---

## üáÆüáπ Versione Italiana

Un sistema AI su Raspberry Pi che riconosce oggetti in tempo reale e genera musica coerente tramite modelli di intelligenza artificiale, funzionando completamente offline.

### Caratteristiche principali

- Riconoscimento oggetti in tempo reale con PyTorch  
- Conversione dei modelli Magenta in TensorFlow Lite per esecuzione locale  
- Generazione musicale basata su oggetti rilevati  
- Intero sistema offline su Raspberry Pi 4  

### Installazione & Uso

1. Clona il repository  
   ```bash
   git clone https://github.com/ninuxi/ai-audio-vision-lab.git
   cd ai-audio-vision-lab
   ```

2. Installa le dipendenze Python
   ```bash
   pip install -r requirements.txt
   ```

3. Inserisci la microSD nel Raspberry Pi 4

4. Esegui lo script
   ```bash
   python3 main.py
   ```

5. Inquadra un oggetto: verr√† generata una melodia coerente.

### Contatti

- **Email**: oggettosonoro@gmail.com
- **GitHub**: [@ninuxi](https://github.com/ninuxi)

Per la documentazione completa in italiano, vedi [docs/README_IT.md](docs/README_IT.md).